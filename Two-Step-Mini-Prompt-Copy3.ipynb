{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0560b4c-34c0-4a01-b670-1e7dcb95cb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "\n",
    "words = [\"Optimistic\",\n",
    "         \"Euphoric\",\n",
    "         \"Liberating\",\n",
    "         \"Heartwarming\",\n",
    "         \"Romantic\",\n",
    "         \"Seductive\",\n",
    "         \"Triumphant\",\n",
    "         \"Peaceful\",\n",
    "         \"Inspiring\",\n",
    "         \"Depressing\",\n",
    "         \"Heartbreaking\",\n",
    "         \"Defeated\",\n",
    "         \"Somber\",\n",
    "         \"Bittersweet\",\n",
    "         \"Angry\",\n",
    "         \"Emotional\",\n",
    "         \"Tense\",\n",
    "         \"Mysterious\",\n",
    "         \"Lighthearted\",\n",
    "         \"Fun\",\n",
    "         \"Lonely\",\n",
    "         \"Disturbing\",\n",
    "         \"Scary\",\n",
    "         \"Thrilling\",\n",
    "         \"Dramatic\",\n",
    "         \"Sincere\",\n",
    "         \"Funny\",\n",
    "         \"Frenzied\",\n",
    "         \"Boyish\",\n",
    "         \"Mature\"]\n",
    "\n",
    "\n",
    "def initialize_chain():\n",
    "    \"\"\"Initialize or reinitialize the conversation chain with Ollama.\"\"\"\n",
    "    global conversation_chain, conversation_memory\n",
    "    conversation_memory = ConversationBufferMemory()\n",
    "    conversation_chain = ConversationChain(\n",
    "    llm=ollama,\n",
    "    memory=conversation_memory,\n",
    "    verbose=False)\n",
    "    return conversation_chain\n",
    "\n",
    "global words_str\n",
    "words_str = \"\"\n",
    "for word in words:\n",
    "    words_str = words_str + \", \" + word\n",
    "\n",
    "words_str = words_str[2:]\n",
    "words_str = words_str.upper()\n",
    "words_str\n",
    "#print(len(words))\n",
    "for value in words:\n",
    "    count = 0\n",
    "    for i in words:\n",
    "        if (i == value):\n",
    "            count = count + 1\n",
    "    #print(value, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31c8aca-9e28-4b03-841d-8961f6f82dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prompts\n",
    "\n",
    "## analyze the movie\n",
    "def prompt1():\n",
    "    return f\"\"\"Based on the character development, music soundtrack, and major plot points, how would you describe the sentiment \n",
    "of the movie {movie}? What emotions dominate the movie? How do viewers feel after watching? Here are some words to consider in your output:\n",
    "{words_str}\n",
    "IMPORTANT: RESPOND IN 350 WORDS OR LESS.\"\"\"\n",
    "\n",
    "## choose one side\n",
    "def prompt2(word, memory):\n",
    "    return f\"\"\"{memory}\n",
    "Based on this analysis, is this movie more {word} than the average movie? Answer in 1 word ONLY using yes or no.\"\"\"\n",
    "\n",
    "def prompt3(word, memory):\n",
    "    return f\"\"\"{memory}\n",
    "\n",
    "\n",
    "YOUR GOAL: \n",
    "Based on the analysis above, on a scale from 0.0001 to 0.9999, how {word} is this movie? \n",
    "IMPORTANT: Respond with ONLY ONE numerical value with 4 decimal points. DO NOT INCLUDE ANY OTHER WORDS OR COMMENTARY.\"\"\"\n",
    "\n",
    "## shortened prompts\n",
    "\n",
    "def prompt1short():\n",
    "    return f\"\"\"Based on the character development, music soundtrack, and major plot points, how would you describe the sentiment \n",
    "of the movie {movie}? What emotions dominate the movie? How do viewers feel after watching?\"\"\"\n",
    "\n",
    "def prompt2short(word):\n",
    "    return f\"\"\"Based on this analysis, is this movie more {word} than the average movie? Answer in 1 word ONLY using yes or no.\"\"\"\n",
    "\n",
    "def prompt3short(word):\n",
    "    return f\"\"\"Based on the analysis above, on a scale from 0.0001 to 0.9999, how {word} is this movie? \n",
    "IMPORTANT: Respond with ONLY ONE numerical value with 4 decimal points.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cd0263f-8ea9-4f6e-96cd-ff01fb6eaa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define analyze_question\n",
    "def analyze_question():\n",
    "    # Initialize the chain if not already done\n",
    "    global conversation_chain\n",
    "    if conversation_chain is None:\n",
    "        initialize_chain()\n",
    "        \n",
    "    comprehensive_prompt = prompt1()\n",
    "    \n",
    "    # Get response using the conversation chain, which maintains history\n",
    "    model_start_time = time.time()\n",
    "\n",
    "    #print(word)\n",
    "    \n",
    "    print(\"\\nRAW MODEL RESPONSE:\")\n",
    "    \n",
    "    # Replace the single predict call with a streaming approach\n",
    "    full_response = \"\"\n",
    "    \n",
    "    for token in conversation_chain.llm.stream(comprehensive_prompt):\n",
    "        #print(token, end=\"\", flush=True)  # Print each token as it arrives\n",
    "        full_response += token\n",
    "    \n",
    "    print()  # Add a newline after streaming completes\n",
    "    \n",
    "    model_end_time = time.time()\n",
    "    #print(f\"Analysis complete (Model processing took {model_end_time - model_start_time:.2f} seconds)\")\n",
    "\n",
    "    conversation_memory.save_context({\"input\": prompt1short()}, {\"output\": full_response})\n",
    "    \n",
    "    # Store the full_response for parsing\n",
    "    response = full_response\n",
    "\n",
    "    return model_start_time - model_end_time, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39feb892-b21f-4cc6-a98c-fcd0f1a1632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define pick_word\n",
    "\n",
    "def pick_word(word):\n",
    "\n",
    "    memory = conversation_memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "    \n",
    "    comprehensive_prompt = prompt2(word, memory)\n",
    "    model_start_time = time.time()\n",
    "    \n",
    "    #print(\"\\nRAW MODEL RESPONSE:\")\n",
    "    \n",
    "    # Replace the single predict call with a streaming approach\n",
    "    full_response = \"\"\n",
    "    \n",
    "    for token in conversation_chain.llm.stream(comprehensive_prompt):\n",
    "        #print(token, end=\"\", flush=True)  # Print each token as it arrives\n",
    "        full_response += token\n",
    "    \n",
    "    #print()  # Add a newline after streaming completes\n",
    "    \n",
    "    model_end_time = time.time()\n",
    "    print(f\"Analysis complete (Model processing took {model_end_time - model_start_time:.2f} seconds)\")\n",
    "\n",
    "    shortened_prompt2 = prompt2short(word)\n",
    "\n",
    "    conversation_memory.save_context({\"input\": shortened_prompt2}, {\"output\": full_response})\n",
    "\n",
    "    return model_start_time - model_end_time, full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72fa4fd-e7fa-4167-9859-0ec32abd7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define give_score\n",
    "def give_score(word, memory):\n",
    "\n",
    "    #memory = conversation_memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "    \n",
    "    comprehensive_prompt = prompt3(word, memory)\n",
    "    model_start_time = time.time()\n",
    "    \n",
    "    # Replace the single predict call with a streaming approach\n",
    "    full_response = \"\"\n",
    "    \n",
    "    for token in conversation_chain.llm.stream(comprehensive_prompt):\n",
    "        #print(token, end=\"\", flush=True)  # Print each token as it arrives\n",
    "        full_response += token\n",
    "    \n",
    "    #print()  # Add a newline after streaming completes\n",
    "    \n",
    "    model_end_time = time.time()\n",
    "    #print(f\"Analysis complete (Model processing took {model_end_time - model_start_time:.2f} seconds)\")\n",
    "\n",
    "    global scoretimes\n",
    "    scoretimes.append(model_end_time - model_start_time)\n",
    "\n",
    "    shortened_prompt = prompt3short(word)\n",
    "\n",
    "    #print(shortened_prompt)\n",
    "\n",
    "    conversation_memory.save_context({\"input\": shortened_prompt}, {\"output\": full_response})\n",
    "\n",
    "    return model_start_time - model_end_time, full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "460e1b05-7969-4616-bcea-cef8f0c9acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW MODEL RESPONSE:\n",
      "\n",
      "\n",
      "TOTAL ESSAY TIME: 36.14178109169006\n",
      "TOTAL GENERATION TIME: 92.57302594184875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def home(gpu):\n",
    "## trying different models\n",
    "    global ollama\n",
    "    # Global memory that will persist between function calls\n",
    "    ollama = Ollama(\n",
    "        model=\"llama3.2:latest\",\n",
    "        num_gpu = gpu,\n",
    "        num_ctx = 2048\n",
    "    )\n",
    "    conversation_memory = ConversationBufferMemory()\n",
    "    conversation_chain = None\n",
    "    \n",
    "    ### PASS THE MOVIE STRING HERE ###\n",
    "    global movie\n",
    "    movie = \"The Matrix (1999)\"\n",
    "    \n",
    "    global scoretimes\n",
    "    scoretimes = []\n",
    "        \n",
    "    initialize_chain()\n",
    "    \n",
    "    # Initialize the conversation chain\n",
    "    \n",
    "    all_words = words\n",
    "    \n",
    "    words_list = []\n",
    "    score_list = []\n",
    "    \n",
    "    fulltime1 = time.time()\n",
    "    \n",
    "    prompt_time, movieanalysis = analyze_question()\n",
    "    print(conversation_memory.load_memory_variables({}).get(\"history\"))\n",
    "\n",
    "    essay_time = time.time() - fulltime1\n",
    "    \n",
    "    print(\"TOTAL ESSAY TIME:\", essay_time)\n",
    "    \n",
    "    for i in range(len(all_words)):\n",
    "        totaltimestart = time.time()\n",
    "        conversation_memory = ConversationBufferMemory()\n",
    "        conversation_chain = None\n",
    "        initialize_chain()\n",
    "        conversation_memory.save_context({\"input\": prompt1()}, {\"output\": movieanalysis})\n",
    "        \n",
    "        \n",
    "        word = all_words[i]\n",
    "    \n",
    "        #prompt_time, response = pick_word(word)\n",
    "    \n",
    "        ## first score\n",
    "        memory = conversation_memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "    \n",
    "        for j in range(3):\n",
    "            prompt_time, response = give_score(word, memory)\n",
    "            words_list.append(word)\n",
    "            score_list.append(response)\n",
    "    \n",
    "        total_time = time.time() - fulltime1\n",
    "    \n",
    "        #print(\"TOTAL TIME:\", totaltimestart - totaltimeend)\n",
    "    \n",
    "    print(\"TOTAL GENERATION TIME:\", total_time)\n",
    "\n",
    "    \n",
    "    \n",
    "    return essay_time, total_time, movieanalysis, words_list, score_list\n",
    "\n",
    "essay_time, total_time, movieanalysis, words_list, score_list = home(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbe5cd84-1a19-461a-8950-cb5a8204f077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Sincere', 'Thrilling', 'Euphoric', 'Dramatic', 'Inspiring', 'Tense', 'Mature', 'Liberating', 'Triumphant', 'Optimistic'\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_numbers(text):\n",
    "    try:\n",
    "        return float(re.findall(r'\\d+\\.?\\d*', text)[0])\n",
    "    except Exception:\n",
    "        return -1\n",
    "\n",
    "num_gpu = 32\n",
    "data2 = {\"gpu\":[], \"essaytime\": [], \"totaltime\": [], \"summary\": [], \"sentiment\":[] }\n",
    "df2 = pd.DataFrame(data2)\n",
    "score_list_llama3 = []\n",
    "for score in score_list:\n",
    "    score_list_llama3.append(extract_numbers(score))\n",
    "data = {\"words\": words_list, \"scores\": score_list_llama3}\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sort_values(by = [\"scores\"], ascending = False)\n",
    "ranked_words = df.drop(df[df.scores > 1].index).drop(df[df.scores < 0].index).groupby('words').agg('mean').sort_values(by = [\"scores\"], ascending = False)\n",
    "top10 = str(ranked_words.index[0:10].tolist())[1:-1]\n",
    "summarywords = len(movieanalysis.split(\" \"))\n",
    "\n",
    "## returns words\n",
    "print(str(ranked_words.index[0:10].tolist())[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258444de-0730-44d9-8e89-cc07d20c23e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
